---
type: breadcrumb
title: "Think: Session Context Preservation Strategy - Capturing compacted sessions for prompt improvement"
topic: Session Context Preservation Strategy - Capturing compacted sessions for prompt improvement
status: concluded
created: "2026-01-21T23:17:45.375Z"
updated: "2026-01-22T00:03:07.152Z"
tags:
  - think
  - concluded
scope: project
conclusion: "DECISION: Implement Decision Point Index (Hybrid Approach D). During PreCompact, parse .jsonl transcripts to extract decision points (AskUserQuestion usage, user questions, architectural choices). Store as lightweight JSON index (~200-300 bytes per point) with context: question, options considered, choice made, why. At prompt improvement time, keyword-search this index and inject matched decision segments + 3-4 surrounding lines for narrative context. This preserves nuance without full transcript bloat, uses existing data, and is queryable without embeddings."
promotedTo: decision-session-context-preservation-strategy-capturing-co
---

# Session Context Preservation Strategy - Capturing compacted sessions for prompt improvement

_Thinking document created 2026-01-21T23:17:45.375Z_

## Thoughts

### 2026-01-21T23:17:53.261Z - Thought
The problem has two dimensions: (1) capturing context from sessions that just compacted (real-time), and (2) accessing historical session context (retrospective). Currently, PreCompact hooks run memory-capture and retrospective agents which create memories, but these are atomic learnings - not full session narratives.

### 2026-01-21T23:17:59.911Z - Thought
What we have now: The .jsonl transcript files contain full session history. After compaction, a summary is written to the transcript. The memory system captures key decisions/learnings/gotchas. Handover documents capture work-in-progress state. But none of these are optimised for *prompt improvement context injection*.

### 2026-01-21T23:18:06.175Z - Counter-argument
Session summaries are huge - a typical compaction summary is 2000+ words. Injecting full session history into every prompt would blow token budgets. We need a compressed, queryable format that captures *what matters* for prompt context, not everything.

### 2026-01-21T23:18:12.683Z - Alternative
Approach A - Session Summaries Index: Create a structured index of session summaries with metadata (date, branch, feature, key topics). At prompt improvement time, query the index by keywords to find relevant sessions, then inject only the matched session context. Similar to how dynamic discovery works now.

### 2026-01-21T23:18:19.406Z - Alternative
Approach B - Micro-Summaries: During PreCompact, generate a 3-5 sentence micro-summary focused on: (1) what was being worked on, (2) current state, (3) key blockers/decisions. Store these in a dedicated sessions/ directory with timestamps. These are small enough to include multiple in prompt context.

### 2026-01-21T23:18:25.653Z - Alternative
Approach C - Embedding-Based Retrieval: Parse .jsonl transcripts, chunk them, generate embeddings, store in a vector index. At prompt time, embed the user's query and retrieve semantically similar session chunks. This gives the most relevant context but requires embedding infrastructure.

### 2026-01-21T23:18:37.564Z - Counter-argument
Critical challenge: Do we actually need session history for prompt improvement? The current memory system already captures the *distilled* knowledge (decisions, learnings, gotchas). Session context is mostly noise - tool calls, file reads, debugging. What unique value does raw session history provide that memories don't?

### 2026-01-21T23:23:24.258Z - Thought
GOTCHA DISCOVERED: The --call claude feature fails inside sandboxed sessions because child Claude CLI processes cannot write their session transcripts to ~/.claude/projects/ (blocked by EROFS). This needs fixing - either by adding that path to sandbox allow-list, or by having child processes use --no-session-persistence flag.

### 2026-01-21T23:25:14.792Z - Thought
FIX IDENTIFIED: In ai-invoke.ts line 115, the code passes --session-id but not --no-session-persistence. Child Claude CLI processes try to write session files to ~/.claude/projects/ which sandbox blocks. Fix: add args.push('--no-session-persistence') to buildCliArgs().

### 2026-01-22T00:02:00.465Z - Thought (model:haiku agent:typescript-expert [6edd7695-2237-4a78-b823-ab75d20b38b8])
The embedding-based retrieval approach (Approach C) seems overcomplicated when the real problem is simpler: we need *searchable context about what was happening* when a session ended, not semantic similarity matching.

The counter-argument is the strongest point here—memories already capture distilled knowledge. But there's a gap: memories don't capture *context about the work itself*. When you're improving a prompt for "TypeScript type inference issues", you need to know: what specific type inference patterns did we encounter? What constraints did we hit? What trade-offs were rejected and why?

Micro-summaries (Approach B) solve this better than either indexing or embeddings because they're designed with *prompt injection* as the primary use case. The 3-5 sentence format is perfect—compact enough to include multiple sessions in context, but detailed enough to trigger relevant memories.

Critical addition to Approach B: Include a "prompts tested" field. Each micro-summary should note which prompts were tried and their results. This directly feeds prompt improvement—you can see what framing worked or failed in similar contexts. This turns session history from passive context into active improvement data.

The sandbox EROFS gotcha is real and blocking, but it's orthogonal to this architecture question. Fix that first (add --no-session-persistence), test that child Claude processes work, then implement micro-summaries. The order matters because you can't validate --call claude improvements until child sessions persist properly.

One more consideration: micro-summaries need timestamps *and* branch names. Prompt improvements often target specific features or codebases. Being able to filter sessions by "work on TypeScript types in claude-prompt-improver" vs "work on TypeScript types in some other project" makes the context vastly more useful.

### 2026-01-22T00:02:27.355Z - Counter-argument (model:haiku agent:performance-optimisation-expert [4d12a76a-5e2a-4add-a6ff-7d8ea668c96a])
Micro-summaries look elegant on paper until you actually need to *use* them. The constraint of 3-5 sentences forces you to discard exactly the context that would help prompt improvement. Consider: you're working on a bug in the authentication flow, you hit a weird edge case with token expiration, you found a workaround but flagged it as technical debt. A micro-summary captures "fixed auth bug, token expiration workaround pending refactor" - but the prompt improvement system needs to know *why* you chose that workaround, what alternatives you considered, what the actual constraints were. That nuance lives in the session transcript, not the summary.

The bigger weakness: micro-summaries assume you know what matters *when you write them*. During PreCompact, you're tired, context-switching, trying to wrap up. You summarise based on what feels important in that moment. But three weeks later when you're trying to improve prompts for a different feature, you discover that throwaway comment about "this pattern appears everywhere in the codebase" would have been critical context - except it got compressed out because it wasn't top-of-mind during compaction.

Also, the "dedicated sessions/ directory with timestamps" creates a new scattered knowledge source. Now you've got memories in `.claude/memory/`, handover state in compaction summaries, AND session micro-summaries somewhere else. When someone (or you) needs to understand what happened, they're hunting across three places. The .jsonl transcripts are already there, already complete, already timestamped. The real problem isn't *capturing* session context - it's *querying* it efficiently without storing duplicate, semi-useful summaries.

### 2026-01-22T00:02:55.340Z - Alternative (model:haiku agent:nodejs-expert [565ff6c6-ed2f-4b99-984d-de11c362d85e])
The .jsonl transcripts are already structured, timestamped, and contain tool outputs. Rather than generating new artifacts or embeddings, what if we treat them as a queryable log that we selectively compress?

Here's the hybrid: During PreCompact, parse the .jsonl file and extract only the *decision points* - moments where the user asked a question, the assistant used AskUserQuestion, or significant architectural choices were made. Tag each with context: what was the question, what were the options considered, what was chosen and why. Store these as a lightweight JSON index file (maybe 200-300 bytes per decision point).

Then at prompt improvement time, the system can search this index by keywords without parsing the full transcript. If a match is found, only *that specific decision segment* gets injected into the prompt context, along with maybe the 3-4 surrounding lines from the transcript for narrative continuity.

This solves the performance-optimisation counter-argument: you're not forcing a micro-summary that discards nuance, but you're also not injecting the full 2000-word transcript. You're injecting the exact granular decision moment that's relevant, with just enough surrounding context to understand why it mattered.

The infrastructure cost is minimal - just a structured index built from data you already have. It's searchable without embeddings. It scales because you're only storing decision points, not full session history. And it gives prompt improvement exactly what it needs: *why was this choice made in this context*, not a generic summary of what happened.

### 2026-01-22T00:03:07.152Z - Conclusion
DECISION: Implement Decision Point Index (Hybrid Approach D). During PreCompact, parse .jsonl transcripts to extract decision points (AskUserQuestion usage, user questions, architectural choices). Store as lightweight JSON index (~200-300 bytes per point) with context: question, options considered, choice made, why. At prompt improvement time, keyword-search this index and inject matched decision segments + 3-4 surrounding lines for narrative context. This preserves nuance without full transcript bloat, uses existing data, and is queryable without embeddings.
