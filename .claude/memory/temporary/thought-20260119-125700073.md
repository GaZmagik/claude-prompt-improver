---
type: breadcrumb
title: "Think: Potential new features for Claude Prompt Improver Plugin"
topic: Potential new features for Claude Prompt Improver Plugin
status: concluded
created: "2026-01-19T12:57:00.086Z"
updated: "2026-01-19T13:05:28.858Z"
tags:
  - think
  - concluded
scope: project
conclusion: |-
  **Decision: Implement Visibility-First MVP for Prompt Improver Plugin**

  **Core Problem Identified:**
  Plugin hooks execute but prompt improvements are invisible - users cannot diagnose whether improvement was skipped (bypass conditions) or failed silently (classification/generation errors).

  **Critical Constraint Clarification:**
  The <100ms hook latency budget is incompatible with LLM-based prompt improvement. Classification + improvement generation will take 2-30s depending on complexity. The plugin must embrace async/visible feedback rather than attempting transparent synchronous improvement.

  **MVP Features (Priority Order):**

  1. **Explicit Improvement Report** - Show what happened:
     - Improvement status: skipped/attempted/applied
     - If skipped: specific bypass reason (short prompt, forked session, low context)
     - If attempted: token count before/after, 3-bullet summary of changes
     - If failed: error type and troubleshooting hint

  2. **Structured Logging** - Debug-friendly output:
     - Log to .claude/logs/prompt-improver-<timestamp>.log
     - Include: classification result, improvement strategy, execution timing, bypass decision logic
     - Avoid logging full prompt content (security/privacy concern)
     - Use log levels: ERROR (failures), INFO (decisions), DEBUG (trace)

  3. **Force Improvement Toggle** - Simple override:
     - Single config: `force_improve: boolean`
     - When true: bypasses all heuristic checks
     - Allows testing whether improvement works at all vs bypass logic preventing it

  **Implementation Priorities:**
  1. Visibility (solves immediate diagnosis problem)
  2. Control (enables experimentation)
  3. Performance optimisation (only after visibility shows where time is spent)

  **Security Safeguards:**
  - Never log full prompt text (summarise: length, first 50 chars, classification)
  - Use structured JSON logs for parsing, not freeform text
  - Implement log rotation to prevent disk exhaustion

  **Next Steps:**
  Enter plan mode to design implementation approach for visibility MVP.
promotedTo: decision-potential-new-features-for-claude-prompt-improver-
---

# Potential new features for Claude Prompt Improver Plugin

_Thinking document created 2026-01-19T12:57:00.086Z_

## Thoughts

### 2026-01-19T12:57:12.325Z - Thought
Initial observation from testing: Plugin installed successfully and hooks executed (we saw UserPromptSubmit hook success messages), but the prompt was not improved before sending to Claude. This suggests the plugin may be bypassing improvement or failing silently. Possible causes: bypass conditions triggered (short prompt, low context, forked session detection), classification returned NONE, or improvement failed/timed out without visible error.

### 2026-01-19T12:58:24.301Z - Thought (model:haiku [e9b8ef9f-6fe2-4fb6-9efb-6e1e2a8f01d0])
**Feature Category Analysis - Debugging & Visibility:**

The core problem from initial testing is that hooks executed but no prompt improvement occurred—a black-box scenario preventing diagnosis. This suggests the strategic starting point should be **debugging and visibility features**.

**Debugging/Troubleshooting (Highest Priority):**
- Verbose logging showing *why* improvements were skipped (classification returned what? bypass condition triggered? token limits? timeout?)
- Hook execution trace through each stage: classification → strategy selection → improvement generation → result application
- Fallback visibility: explicitly state *why* a prompt bypassed improvement (not a question, session type mismatch, prompt too short)
- Performance metrics: classification/generation latency, token usage, strategy selection reasoning

This directly solves the initial observation problem—users can't debug a silent failure.

**Configuration/Customization (Medium Priority):**
Strategy weighting (clarity vs conciseness), bypass condition tuning, context source prioritization, output format options—these let power users adapt the plugin to their workflow.

**New Context Sources (Medium-Long Term):**
Session history patterns, user style profiles, codebase-aware context, model-specific improvements—higher effort but increase relevance over time.

**Improvement Strategies (Implementation Depth):**
Multi-pass refinement, contextual examples, constraint-aware generation, feedback loops—compound the core improvement quality.

**User Feedback Mechanisms (Usage Data):**
Acceptance tracking, satisfaction voting, specific feedback channels, A/B alternatives—create a learning loop.

The strategic ordering matters: users can't effectively use the plugin without understanding its behaviour, so debugging features unlock the others.

### 2026-01-19T12:58:58.099Z - Counter-argument (model:haiku agent:security-code-expert [1dbfc5ba-146a-405b-94ca-3022266b32c0])
I'll provide the counter-argument directly:

---

**Counter-perspective: Feature implementation risks and downsides**

**Verbose Logging Privacy Exposure:**
- Logging prompt content for debugging creates a security liability — user prompts often contain sensitive information, proprietary code, personal data, or confidential business logic
- Persisting logs to disk (.claude/logs/) means sensitive data survives the session and could be exposed via file access, backups, or system compromise
- Users won't necessarily understand what data is being logged or how to clean it up, creating compliance risks (GDPR, data retention policies)

**Configuration Complexity Trap:**
- Adding multiple context sources (git history, file structure, recent changes) seems helpful but creates a configuration management nightmare
- Each source adds latency — users will report slow prompt submission times without understanding why
- Misconfigured sources could bloat prompts unnecessarily, using token budget inefficiently and potentially exceeding context limits
- Users will need documentation and troubleshooting guides, increasing maintenance burden

**Improvement Strategy Interference:**
- Multiple competing improvement strategies (clarity enhancement, structure optimisation, pattern matching) risk producing contradictory changes
- Some strategies could 'improve' a prompt in ways that harm its intent — e.g., adding formality to a deliberately casual prompt, or restructuring a carefully-ordered sequence
- Users won't know which strategy was applied, making it impossible to predict or control improvements

**Performance Degradation:**
- Classification + improvement pipeline for every prompt adds latency to every session interaction
- In fast-paced work sessions, even 200–500ms delays become annoying
- Cold starts (loading models, initialising systems) could cause noticeable hangs on first use

**Feedback Loop Privacy & Misuse Risk:**
- Collecting 'did this help?' feedback creates liability — users might report on prompts containing sensitive data
- Feedback dataset becomes a treasure trove of real user prompts for training data
- Building improvement algorithms from feedback means encoding user expectations into the plugin, potentially perpetuating problematic patterns

**Alternative consideration:** The plugin's current silent operation might actually be a feature, not a bug. Users get improvement attempts without cognitive overhead or distraction. The fact that improvement didn't occur might be the system correctly identifying that this particular prompt didn't need improvement.

### 2026-01-19T12:59:36.403Z - Thought (model:haiku agent:ui-ux-design-expert [14a940fa-bfb4-4d8b-a5c6-11f754b76c47])

### 2026-01-19T13:00:08.179Z - Alternative (model:haiku [ad243c41-4591-4249-89d2-fa1db5504958])
**MVP: Visibility & Control First**

Strip away the enhancement complexity entirely. The core problem isn't that the plugin needs smarter features—it's that it runs silently and users can't tell if anything happened.

**Three-feature MVP:**

1. **Explicit improvement report** – After improvement completes, show a hook message with:
   - Token count before/after
   - Three bullet-point summary of changes made ("Simplified technical jargon", "Added concrete examples", "Clarified intent")
   - This makes the improvement visible and verifiable

2. **Bypass reason logging** – When the plugin skips improvement (short prompt, forked session, low context), log the specific reason:
   - "Improvement skipped: prompt below minimum threshold"
   - "Improvement skipped: forked session detected"
   - Users understand why nothing happened instead of assuming it's broken

3. **Single force override** – One config option: `force_improve=true`
   - Overrides all bypass logic
   - Replace complex per-condition controls with this single toggle
   - Users who want improvement regardless can enable it

**Why this works:**
- Solves the immediate problem (silent failures and invisible behavior)
- ~2-3 hours of implementation work, not days
- Provides real usage data to inform future features
- Every piece is observable and debuggable
- Shifts focus from complexity to clarity

The streaming, async processing, and granular control options can come later once you have evidence users actually need them. Right now, the bug is invisibility, not missing features.

### 2026-01-19T13:00:55.457Z - Thought (model:haiku agent:performance-optimisation-expert [ce20a7ed-ba7d-405e-97e4-249b2c6293dd])
I've prepared a detailed performance analysis for the Claude Prompt Improver Plugin feature deliberation. The analysis addresses your specific request to evaluate latency budgets, caching strategies, and resource usage across the feature categories being considered.

**Key insights:**

1. **Hook execution latency is the hard constraint** - synchronous UserPromptSubmit hooks must complete in <100ms to maintain responsive CLI experience

2. **Caching is non-negotiable** - expensive operations (context gathering, LLM classification) need aggressive caching strategies, either session-level (fast, limited scope) or persistent (survives sessions, complex invalidation)

3. **Three viable strategies with different trade-offs:**
   - Async processing: Non-blocking but user can't see improvements
   - Sync with heavy caching: Responsive when cached, but cold starts hit 1-2s
   - Smart bypass with heuristics: Most prompts avoid expensive analysis, maintaining <10ms latency

4. **Resource concerns** - persistent caching, LLM API call costs, and memory accumulation in long sessions need careful design

The analysis recommends prioritising Option C (heuristic-based classification + selective processing) for MVP phase, establishing empirical evidence of which improvements justify the latency cost before investing in complex caching infrastructure.

Would you like me to add this to the project's permanent memory store, or continue exploring other feature implications?

### 2026-01-19T13:01:57.847Z - Thought (model:haiku agent:typescript-expert [f3250237-7c7b-4210-999c-0238adf8989f])
**Code Quality & Maintainability Analysis**

**Testing Strategy Implications:**

1. **Hook Behavior Testing** - Most complex aspect:
   - UserPromptSubmit hooks execute in isolation; mocking the entire Claude API response chain is non-trivial
   - Visibility features (logging, metrics) require integration tests with actual hook execution to verify output appears in correct channels (stdout, files, telemetry)
   - Known gotcha: `vi.mock()` pollution across test files makes mock state management fragile - need strict test isolation or beforeEach/afterEach reset
   - Features like bypass conditions need parametrised tests to cover all decision paths (low-context, short-prompt, forked-session detection)
   - Result: Visibility/control features require more robust integration test infrastructure; performance features can rely more on unit tests

2. **Configuration Schema Evolution** - Technical debt risk:
   - Current config likely supports bypass thresholds; adding new features (verbosity levels, output targets, performance profiles) requires backward-compatible schema evolution
   - TypeScript union types work well for versioning (e.g., `ConfigV1 | ConfigV2`), but require migration logic
   - If quick wins add adhoc config properties, technical debt accumulates fast; robust solution needs schema validation at startup

**TypeScript Type Safety Considerations:**

- **Enum vs Union Types**: Verbosity levels, output targets should use string literal unions, not enums - better tree-shaking and clearer inference
- **Discriminated Unions**: Configuration variants (silent mode, debug mode, metrics mode) should use discriminated unions to ensure type-safe conditional behaviour
- **Utility Types for Config**: Use `Pick<Config, 'bypass' | 'timeout'>` rather than creating new types, reduces duplication
- **Generic Constraints**: Hook response handler should use generics with `extends { success: boolean; data?: unknown }` to type-safely handle different response shapes

**Error Handling Patterns - Critical Differentiator:**

- **Visibility/Control features** require explicit error handling: if logging fails, should it fail the entire prompt improvement? Needs Result type pattern rather than throwing exceptions
- **Performance features** need graceful degradation: if cache miss occurs, fall back to full improvement without user seeing latency impact
- **Configuration features** should use discriminated union errors: `{ type: 'schema-validation'; issues: ValidationError[] } | { type: 'runtime'; message: string }`

**Backwards Compatibility Strategy:**

- Current plugin.json manifest likely has fixed schema; adding new config fields needs default values to avoid breaking existing installations
- Hook execution order matters (memory check gotcha shows this): if adding new hooks, must ensure they don't conflict with existing ones
- Consider feature flags: wrap new features in `config.experimental.*` to separate stable from beta functionality

**Technical Debt vs Robust Solution Trade-off:**

**Quick wins (high debt):**
- Adding verbosity levels as direct config properties with if/else chains
- Logging to stdout without abstraction - breaks in non-terminal environments
- Hardcoded performance thresholds with no parameterisation

**Robust solutions (lower debt):**
- Abstract logging behind interface: `interface Logger { log(level: 'debug'|'info'|'warn'|'error', msg: string) }`
- Configuration factory pattern: separate parsing logic from usage
- Performance metrics as injectable dependencies - easier to test and extend

**Recommendation Priority Order:**

1. **Start with Visibility** - Lower implementation complexity, highest user value, easier test story
   - Define Logger interface first (enables proper abstraction)
   - Ensure hook error handling is Result-typed (prevents silent failures)
   - Requires: ~10 test files, config schema bump

2. **Then Control** - Builds on visibility infrastructure, moderate complexity
   - Reuses Logger patterns
   - Configuration schema now stable from Visibility phase
   - Bypass conditions have proven test patterns

3. **Performance last** - Highest risk for complexity without visibility features providing context
   - Benefits from metrics infrastructure established in Visibility phase
   - Caching adds state management complexity; easier to test once logging/monitoring in place

### 2026-01-19T13:05:28.857Z - Conclusion
**Decision: Implement Visibility-First MVP for Prompt Improver Plugin**

**Core Problem Identified:**
Plugin hooks execute but prompt improvements are invisible - users cannot diagnose whether improvement was skipped (bypass conditions) or failed silently (classification/generation errors).

**Critical Constraint Clarification:**
The <100ms hook latency budget is incompatible with LLM-based prompt improvement. Classification + improvement generation will take 2-30s depending on complexity. The plugin must embrace async/visible feedback rather than attempting transparent synchronous improvement.

**MVP Features (Priority Order):**

1. **Explicit Improvement Report** - Show what happened:
   - Improvement status: skipped/attempted/applied
   - If skipped: specific bypass reason (short prompt, forked session, low context)
   - If attempted: token count before/after, 3-bullet summary of changes
   - If failed: error type and troubleshooting hint

2. **Structured Logging** - Debug-friendly output:
   - Log to .claude/logs/prompt-improver-<timestamp>.log
   - Include: classification result, improvement strategy, execution timing, bypass decision logic
   - Avoid logging full prompt content (security/privacy concern)
   - Use log levels: ERROR (failures), INFO (decisions), DEBUG (trace)

3. **Force Improvement Toggle** - Simple override:
   - Single config: `force_improve: boolean`
   - When true: bypasses all heuristic checks
   - Allows testing whether improvement works at all vs bypass logic preventing it

**Implementation Priorities:**
1. Visibility (solves immediate diagnosis problem)
2. Control (enables experimentation)
3. Performance optimisation (only after visibility shows where time is spent)

**Security Safeguards:**
- Never log full prompt text (summarise: length, first 50 chars, classification)
- Use structured JSON logs for parsing, not freeform text
- Implement log rotation to prevent disk exhaustion

**Next Steps:**
Enter plan mode to design implementation approach for visibility MVP.
